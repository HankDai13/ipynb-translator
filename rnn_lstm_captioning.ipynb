{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDJwQPZcupab"
   },
   "source": [
    "# EECS 498-007/598-005 Assignment 5-1: Image captioning with RNNs and LSTMs\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstname LASTNAME, #00000000   //   例如：Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KMxqLt1h2kx"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hello WORLD, #XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Euek3FWn6bhA",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image Captioning with RNNs\n",
    "\n",
    "In this exercise, you will implement vanilla recurrent neural networks (RNNs), [long-short term memory networks (LSTMs)](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory), and [attention-based LSTMs](https://arxiv.org/abs/1409.0473) to train a model that can generate natural language captions for images.\n",
    "\n",
    "Models in this exercise are highly similar to very early works in neural-network based image captioning. If you are interested to learn more, check out these two papers:\n",
    "\n",
    "1. [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)\n",
    "2. [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你好，世界，#XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN进行图像描述生成\n",
    "\n",
    "在本练习中，你将实现标准的循环神经网络（RNN）、[长短期记忆网络（LSTM）](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)以及[基于注意力机制的LSTM](https://arxiv.org/abs/1409.0473)，训练一个能够为图像生成自然语言描述的模型。\n",
    "\n",
    "本练习中的模型与早期基于神经网络的图像描述生成工作高度相似。若想深入了解，可参考以下两篇论文：\n",
    "\n",
    "1. [Show and Tell：一种神经图像描述生成器](https://arxiv.org/abs/1411.4555)\n",
    "2. [Show, Attend and Tell：基于视觉注意力的神经图像描述生成](https://arxiv.org/abs/1502.03044)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzqbYcKdz6ew"
   },
   "source": [
    "## Setup Code\n",
    "\n",
    "Before getting started, we need to run some boilerplate code to set up our environment, same as previous assignments. You\"ll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN进行图像描述生成\n",
    "\n",
    "在本练习中，你将实现基础的循环神经网络（RNN）、[长短期记忆网络（LSTM）](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)以及[基于注意力机制的LSTM](https://arxiv.org/abs/1409.0473)，训练一个能够为图像生成自然语言描述的模型。\n",
    "\n",
    "本练习中的模型与早期基于神经网络的图像描述生成工作高度相似。若你希望深入了解，可参考以下两篇论文：\n",
    "\n",
    "1. [Show and Tell：一种神经图像描述生成器](https://arxiv.org/abs/1411.4555)\n",
    "2. [Show, Attend and Tell：基于视觉注意力的神经图像描述生成](https://arxiv.org/abs/1502.03044)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "28O_qwFfdQpr"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3H9pcnudWlg"
   },
   "source": [
    "### Google Colab Setup\n",
    "\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用RNN生成图像描述\n",
    "\n",
    "在本练习中，你将实现标准循环神经网络（RNN）、长短期记忆网络（LSTM）以及带注意力机制的LSTM，训练一个能为图像自动生成自然语言描述的模型。\n",
    "\n",
    "本练习所用模型与早期基于神经网络的图像描述方法非常相似。如需深入了解，可参考以下两篇论文：\n",
    "\n",
    "1. 《Show and Tell：神经图像描述生成器》\n",
    "2. 《Show, Attend and Tell：基于视觉注意力的神经图像描述生成》"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yv8Z8EiudX25"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sek0GtVOdlKT"
   },
   "source": [
    " Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "[\"eecs598\", \"a5_helper.py\", \"rnn_lstm_captioning.ipynb\",  \"rnn_lstm_captioning.py\", \"Transformers.py\", \"Transformers.ipynb\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置代码\n",
    "\n",
    "在开始之前，我们需要运行一些样板代码来配置环境，与之前的作业相同。每次启动笔记本时，你都需要重新运行这些设置代码。\n",
    "\n",
    "首先，运行此单元格以加载 [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) 扩展。这使我们能够编辑 .py 源文件，并将其重新导入笔记本，从而实现无缝的编辑和调试体验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9t0-bGZdnr8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a WI2022 folder and put all the files under A5 folder, then \"WI2022/A5\"\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "\n",
    "\n",
    "# Add to sys so we can import .py files.\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5LWJPBtdrpZ"
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from rnn_lstm_captioning.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `rnn_lstm_captioning.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFZKi0podzhO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from rnn_lstm_captioning import hello_rnn_lstm_captioning\n",
    "\n",
    "\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "hello_rnn_lstm_captioning()\n",
    "\n",
    "rnn_lstm_path = os.path.join(GOOGLE_DRIVE_PATH, \"rnn_lstm_captioning.py\")\n",
    "rnn_lstm_edit_time = time.ctime(os.path.getmtime(rnn_lstm_path))\n",
    "print(\"rnn_lstm_captioning.py last edited on %s\" % rnn_lstm_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab 环境设置\n",
    "\n",
    "接下来，我们需要在 Google Colab 上运行几条命令来配置环境。如果你是在本地机器上运行此笔记本，则可跳过本节。\n",
    "\n",
    "请运行以下单元格以挂载你的 Google 云端硬盘。点击链接，登录你的 Google 账号（需与存储本笔记本所用的账号一致），然后将弹出窗口中的授权码复制粘贴到下方文本框中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8fTwRpXfwyM"
   },
   "source": [
    "### Load Packages\n",
    "\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q53DlMXboP-T"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\matplotlib\\style\\core.py:137\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:866\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[1;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[0;32m    865\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 866\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:843\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    842\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[1;32m--> 843\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# for plotting\u001b[39;00m\n\u001b[0;32m     13\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseaborn\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Prettier plots\u001b[39;00m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10.0\u001b[39m, \u001b[38;5;241m8.0\u001b[39m)  \u001b[38;5;66;03m# set default size of plots\u001b[39;00m\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfont.size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\matplotlib\\style\\core.py:139\u001b[0m, in \u001b[0;36muse\u001b[1;34m(style)\u001b[0m\n\u001b[0;32m    137\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    143\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from eecs598.grad import compute_numeric_gradient, rel_error\n",
    "from eecs598.utils import attention_visualizer, reset_seed\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn\")  # Prettier plots\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"font.size\"] = 24\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvUDZWGU3VLV"
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrAX9FOLpr9k"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Define some common variables for dtypes/devices.\n",
    "# These can be keyword arguments while defining new tensors.\n",
    "to_float = {\"dtype\": torch.float32, \"device\": DEVICE}\n",
    "to_double = {\"dtype\": torch.float64, \"device\": DEVICE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCPZwvOd6bhF",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# COCO Captions\n",
    "\n",
    "For this exercise we will use the 2014 release of the [COCO Captions dataset](http://cocodataset.org/) which has become the standard testbed for image captioning. The dataset consists of 80,000 training images and 40,000 validation images, each annotated with 5 captions written by workers on Amazon Mechanical Turk.\n",
    "\n",
    "We have preprocessed the data for you already and saved them into a serialized data file. It contains 10,000 image-caption pairs for training and 500 for testing. The images have been downsampled to 112x112 for computation efficiency and captions are tokenized and numericalized, clamped to 15 words. You can download the file named `coco.pt` (378MB) with the link below and run some useful stats.\n",
    "\n",
    "You will later use RegNet-X 400MF model to extract features for the images. A few notes on the caption preprocessing:\n",
    "\n",
    "Dealing with strings is inefficient, so we will work with an encoded version of the captions. Each word is assigned an integer ID, allowing us to represent a caption by a sequence of integers. The mapping between integer IDs and words is saved in an entry named `vocab` (both `idx_to_token` and `token_to_idx`), and we use the function `decode_captions` from `a5_helper.py` to convert tensors of integer IDs back into strings.\n",
    "\n",
    "There are a couple special tokens that we add to the vocabulary. We prepend a special `<START>` token and append an `<END>` token to the beginning and end of each caption respectively. Rare words are replaced with a special `<UNK>` token (for \"unknown\"). In addition, since we want to train with minibatches containing captions of different lengths, we pad short captions with a special `<NULL>` token after the `<END>` token and don't compute loss or gradient for `<NULL>` tokens. Since they are a bit of a pain, we have taken care of all implementation details around special tokens for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set a few constants related to data loading.\n",
    "IMAGE_SHAPE = (112, 112)\n",
    "NUM_WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "# Batch size used for full training runs:\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch size used for overfitting sanity checks:\n",
    "OVR_BATCH_SIZE = BATCH_SIZE // 8\n",
    "\n",
    "# Batch size used for visualization:\n",
    "VIS_BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMok4gFXqjre"
   },
   "outputs": [],
   "source": [
    "from a5_helper import load_coco_captions\n",
    "\n",
    "# Download and load serialized COCO data from coco.pt\n",
    "# It contains a dictionary of\n",
    "# \"train_images\" - resized training images (IMAGE_SHAPE)\n",
    "# \"val_images\" - resized validation images (IMAGE_SHAPE)\n",
    "# \"train_captions\" - tokenized and numericalized training captions\n",
    "# \"val_captions\" - tokenized and numericalized validation captions\n",
    "# \"vocab\" - caption vocabulary, including \"idx_to_token\" and \"token_to_idx\"\n",
    "\n",
    "if os.path.isfile(\"./datasets/coco.pt\"):\n",
    "    print(\"COCO data exists!\")\n",
    "else:\n",
    "    print(\"downloading COCO dataset\")\n",
    "    !wget http://web.eecs.umich.edu/~justincj/teaching/eecs498/coco.pt -P ./datasets/\n",
    "\n",
    "# load COCO data from coco.pt, loaf_COCO is implemented in a5_helper.py\n",
    "data_dict = load_coco_captions(path=\"./datasets/coco.pt\")\n",
    "\n",
    "num_train = data_dict[\"train_images\"].size(0)\n",
    "num_val = data_dict[\"val_images\"].size(0)\n",
    "\n",
    "# declare variables for special tokens\n",
    "NULL_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<NULL>\"]\n",
    "START_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<START>\"]\n",
    "END_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<END>\"]\n",
    "UNK_index = data_dict[\"vocab\"][\"token_to_idx\"][\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载包\n",
    "\n",
    "运行本笔记本的一些设置代码：导入一些有用的包并增大默认图表尺寸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80RW_nSH6bhH"
   },
   "source": [
    "## Look at the data\n",
    "It is always a good idea to look at examples from the dataset before working with it.\n",
    "\n",
    "Run the following to sample a small minibatch of training data and show the images and their captions. Running it multiple times and looking at the results helps you to get a sense of the dataset.\n",
    "\n",
    "Note that we decode the captions using the `decode_captions` function.\n",
    "You can check its implementation in `a5_helper.py`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-oiW9Ut6bhH"
   },
   "outputs": [],
   "source": [
    "from a5_helper import decode_captions\n",
    "\n",
    "\n",
    "# Sample a minibatch and show the reshaped 112x112 images and captions\n",
    "sample_idx = torch.randint(0, num_train, (VIS_BATCH_SIZE, ))\n",
    "sample_images = data_dict[\"train_images\"][sample_idx]\n",
    "sample_captions = data_dict[\"train_captions\"][sample_idx]\n",
    "for i in range(VIS_BATCH_SIZE):\n",
    "    plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    caption_str = decode_captions(\n",
    "        sample_captions[i], data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "    plt.title(caption_str)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2SQMNIH6bhJ"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "As discussed in lecture, we will use Recurrent Neural Network (RNN) language models for image captioning. We will cover the vanilla RNN model first and later LSTM and attention-based language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记本将使用 GPU 加速计算。请运行以下代码以确保已启用 GPU："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XHZMI356bhJ"
   },
   "source": [
    "## Vanilla RNN: step forward\n",
    "\n",
    "First implement the `rnn_step_forward` for a single timestep of a vanilla recurrent neural network.\n",
    "Run the following to check your implementation. You should see errors on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3oU8JJj6bhK"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import rnn_step_forward\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = torch.linspace(-0.4, 0.7, steps=N * D, **to_double).view(N, D)\n",
    "prev_h = torch.linspace(-0.2, 0.5, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-0.1, 0.9, steps=D * H, **to_double).view(D, H)\n",
    "Wh = torch.linspace(-0.3, 0.7, steps=H * H, **to_double).view(H, H)\n",
    "b = torch.linspace(-0.2, 0.4, steps=H, **to_double)\n",
    "\n",
    "\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = torch.tensor(\n",
    "    [\n",
    "        [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "        [0.66854692, 0.79562378, 0.87755553, 0.92795967],\n",
    "        [0.97934501, 0.99144213, 0.99646691, 0.99854353],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"next_h error: \", rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tid-ljPA6bhL"
   },
   "source": [
    "## Vanilla RNN: step backward\n",
    "Then implement the `rnn_step_backward` for a single timestep of a vanilla recurrent neural network. Run the following to numerically gradient check your implementation. You should see errors on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO 图像描述数据集\n",
    "\n",
    "在本练习中，我们将使用 2014 年发布的 [COCO 图像描述数据集](http://cocodataset.org/)，该数据集已成为图像描述任务的标准测试平台。数据集包含 80,000 张训练图像和 40,000 张验证图像，每张图像配有 5 条由亚马逊 Mechanical Turk 平台工作人员撰写的描述。\n",
    "\n",
    "我们已为你预处理好数据，并将其保存为一个序列化的文件。该文件包含 10,000 个用于训练的图像-描述对，以及 500 个用于测试的图像-描述对。为提高计算效率，图像已下采样至 112x112 分辨率，描述文本也已完成分词和数值化处理，并统一截断为最多 15 个词。你可以通过下方链接下载名为 `coco.pt`（378MB）的文件，并运行一些有用的统计分析。\n",
    "\n",
    "后续你将使用 RegNet-X 400MF 模型提取图像特征。关于描述文本的预处理，有几点说明：\n",
    "\n",
    "直接处理字符串效率较低，因此我们将使用编码后的描述版本。每个词被赋予一个整数 ID，从而可以用整数序列表示一条描述。词与 ID 的映射关系保存在名为 `vocab` 的条目中（包含 `idx_to_token` 和 `token_to_idx`），并可通过 `a5_helper.py` 中的 `decode_captions` 函数将整数 ID 张量还原为原始字符串。\n",
    "\n",
    "我们在词汇表中加入了几个特殊标记：每条描述开头添加 `<START>` 标记，结尾添加 `<END>` 标记；罕见词则替换为 `<UNK>`（表示“未知”）。此外，由于我们希望在训练时使用包含不同长度描述的小批量数据，短描述在 `<END>` 后会用 `<NULL>` 标记填充，且在计算损失或梯度时不考虑 `<NULL>` 标记。这些特殊标记的实现细节我们已为你处理妥当，无需额外操心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPyfJofC6bhM"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import rnn_step_backward\n",
    "\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, H = 4, 5, 6\n",
    "x = torch.randn(N, D, **to_double)\n",
    "h = torch.randn(N, H, **to_double)\n",
    "Wx = torch.randn(D, H, **to_double)\n",
    "Wh = torch.randn(H, H, **to_double)\n",
    "b = torch.randn(H, **to_double)\n",
    "\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "\n",
    "dnext_h = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = compute_numeric_gradient(fx, x, dnext_h)\n",
    "dprev_h_num = compute_numeric_gradient(fh, h, dnext_h)\n",
    "dWx_num = compute_numeric_gradient(fWx, Wx, dnext_h)\n",
    "dWh_num = compute_numeric_gradient(fWh, Wh, dnext_h)\n",
    "db_num = compute_numeric_gradient(fb, b, dnext_h)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_step_backward\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_num, dx))\n",
    "print(\"dprev_h error: \", rel_error(dprev_h_num, dprev_h))\n",
    "print(\"dWx error: \", rel_error(dWx_num, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_num, dWh))\n",
    "print(\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjZjH5JW6bhN"
   },
   "source": [
    "## Vanilla RNN: forward\n",
    "Now that you have implemented the forward and backward passes for a single timestep of a vanilla RNN, you will combine these pieces to implement a RNN that processes an entire sequence of data. First implement `rnn_forward` by making calls to the `rnn_step_forward` function that you defined earlier.\n",
    "\n",
    "Run the following to check your implementation. You should see errors on the order of `1e-6` or less.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GQWEn3Z6bhO"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import rnn_forward\n",
    "\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = torch.linspace(-0.1, 0.3, steps=N * T * D, **to_double).view(N, T, D)\n",
    "h0 = torch.linspace(-0.3, 0.1, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-0.2, 0.4, steps=D * H, **to_double).view(D, H)\n",
    "Wh = torch.linspace(-0.4, 0.1, steps=H * H, **to_double).view(H, H)\n",
    "b = torch.linspace(-0.7, 0.1, steps=H, **to_double)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_forward\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.42070749, -0.27279261, -0.11074945, 0.05740409, 0.22236251],\n",
    "            [-0.39525808, -0.22554661, -0.0409454, 0.14649412, 0.32397316],\n",
    "            [-0.42305111, -0.24223728, -0.04287027, 0.15997045, 0.35014525],\n",
    "        ],\n",
    "        [\n",
    "            [-0.55857474, -0.39065825, -0.19198182, 0.02378408, 0.23735671],\n",
    "            [-0.27150199, -0.07088804, 0.13562939, 0.33099728, 0.50158768],\n",
    "            [-0.51014825, -0.30524429, -0.06755202, 0.17806392, 0.40333043],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "print(\"h error: \", rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P570PTsw6bhP"
   },
   "source": [
    "## Vanilla RNN: backward\n",
    "Implement the `rnn_backward` for a vanilla RNN. This should run back-propagation over the entire sequence, making calls to the `rnn_step_backward` function that you defined earlier. You should see errors on the order of `1e-6` or less.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看数据\n",
    "在处理数据集之前，先查看其中的一些样本总是一个好习惯。\n",
    "\n",
    "运行以下代码可以抽取一小批训练数据，并显示对应的图像及其标题。多次运行并观察结果，有助于你更好地了解数据集的特点。\n",
    "\n",
    "注意，我们使用 `decode_captions` 函数对标题进行解码，你可以在 `a5_helper.py` 中查看该函数的具体实现！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny25RusA6bhQ"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import rnn_backward, rnn_forward\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = torch.randn(N, T, D, **to_double)\n",
    "h0 = torch.randn(N, H, **to_double)\n",
    "Wx = torch.randn(D, H, **to_double)\n",
    "Wh = torch.randn(H, H, **to_double)\n",
    "b = torch.randn(H, **to_double)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "# YOUR_TURN: Implement rnn_backward\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = compute_numeric_gradient(fx, x, dout)\n",
    "dh0_num = compute_numeric_gradient(fh0, h0, dout)\n",
    "dWx_num = compute_numeric_gradient(fWx, Wx, dout)\n",
    "dWh_num = compute_numeric_gradient(fWh, Wh, dout)\n",
    "db_num = compute_numeric_gradient(fb, b, dout)\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_num, dx))\n",
    "print(\"dh0 error: \", rel_error(dh0_num, dh0))\n",
    "print(\"dWx error: \", rel_error(dWx_num, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_num, dWh))\n",
    "print(\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEDUmZOkU_LO"
   },
   "source": [
    "## Vanilla RNN: backward with autograd\n",
    "\n",
    "Now we will entirely depend on the PyTorch autograd module (`torch.autograd`) to compute the backward pass of RNN.\n",
    "`torch.autograd` provides classes and functions implementing **automatic differentiation** of arbitrary scalar valued functions.\n",
    "It requires minimal changes to the existing code - if you pass tensors with `requires_grad=True` to the forward function you wrote earlier, you can just call `.backward(gradient=grad)` on the output to compute gradients on the input and weights.\n",
    "\n",
    "**NOTE: We released the PyTorch API walkthrough notebook during the past assignment, it is available on Piazza and the course website.**\n",
    "\n",
    "Now we can compare the manual backward pass with the autograd backward pass.\n",
    "Read the code in following cell, and execute it to compare your implementation with `torch.autograd`.\n",
    "You should get a relative error less than `1e-10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AMXoqNOVRa_"
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "# set requires_grad=True\n",
    "x = torch.randn(N, T, D, **to_double, requires_grad=True)\n",
    "h0 = torch.randn(N, H, **to_double, requires_grad=True)\n",
    "Wx = torch.randn(D, H, **to_double, requires_grad=True)\n",
    "Wh = torch.randn(H, H, **to_double, requires_grad=True)\n",
    "b = torch.randn(H, **to_double, requires_grad=True)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = torch.randn(*out.shape, **to_double)\n",
    "\n",
    "# Manual backward:\n",
    "with torch.no_grad():\n",
    "    dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "# Backward with autograd: the magic happens here!\n",
    "out.backward(dout)\n",
    "\n",
    "dx_auto, dh0_auto, dWx_auto, dWh_auto, db_auto = (\n",
    "    x.grad,\n",
    "    h0.grad,\n",
    "    Wx.grad,\n",
    "    Wh.grad,\n",
    "    b.grad,\n",
    ")\n",
    "\n",
    "print(\"dx error: \", rel_error(dx_auto, dx))\n",
    "print(\"dh0 error: \", rel_error(dh0_auto, dh0))\n",
    "print(\"dWx error: \", rel_error(dWx_auto, dWx))\n",
    "print(\"dWh error: \", rel_error(dWh_auto, dWh))\n",
    "print(\"db error: \", rel_error(db_auto, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环神经网络\n",
    "\n",
    "如课堂所述，我们将使用循环神经网络（RNN）语言模型进行图像描述生成。首先介绍基础的RNN模型，随后再探讨LSTM及基于注意力机制的语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgmxOjX0prA4"
   },
   "source": [
    "## RNN Module\n",
    "\n",
    "We can now wrap the vanilla RNN implementation into a PyTorch module.\n",
    "Recall from the past assignment/tutorial -- `nn.Module` is a base class for all neural network modules in PyTorch. More details regarding its attributes, functions, and methods could be found [in PyTorch documentation](https://pytorch.org/docs/stable/nn.html?highlight=module#torch.nn.Module).\n",
    "\n",
    "In short, the weights and biases are declared in `__init__` and function `forward` will call the `rnn_forward` function from before.\n",
    "The backward function will not be used, and entirely handled by `torch.autograd`.\n",
    "**We have written this part in `RNN` for you but you are highly recommended to go through the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import RNN, rnn_forward\n",
    "\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = torch.randn(N, T, D, **to_double)\n",
    "h0 = torch.randn(N, H, **to_double)\n",
    "\n",
    "rnn_module = RNN(D, H).to(**to_double)\n",
    "\n",
    "# Call forward in module:\n",
    "hn1 = rnn_module(x, h0)\n",
    "\n",
    "# Call without module: (but access weights from module)\n",
    "# Equivalent to above, we won't do this henceforth.\n",
    "Wx, Wh, b = rnn_module.Wx, rnn_module.Wh, rnn_module.b\n",
    "hn2, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "print(\"Output error with/without module: \", rel_error(hn1, hn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础RNN：单步前向传播\n",
    "\n",
    "首先实现基础循环神经网络单个时间步的 `rnn_step_forward` 函数。  \n",
    "运行以下代码以验证你的实现，误差应小于或接近 `1e-8`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIjmnjRd6bhZ"
   },
   "source": [
    "# RNN for image captioning\n",
    "\n",
    "You will implement a few necessary tools and layers in order to build an image captioning model (class `CaptioningRNN`).\n",
    "\n",
    "## Image Feature Extraction\n",
    "\n",
    "The first essential component in an image captioning model is an encoder that inputs an image and produces features for decoding the caption.\n",
    "Here, we use a small [RegNetX-400MF](https://pytorch.org/vision/stable/models.html#torchvision.models.regnet_x_400mf) as the backbone so we can train in reasonable time on Colab. This model is similar to detector backbone seen in the past assignment.\n",
    "\n",
    "It accepts image batches of shape `(B, C, H, W)` and outputs spatial features from final layer that have shape `(B, C, H/32, W/32)`.\n",
    "For vanilla RNN and LSTM, we use the average pooled features (shape `(B, C)`) for decoding captions, whereas for attention LSTM we aggregate the spatial features by learning attention weights.\n",
    "Checkout the `ImageEncoder` method in `rnn_lstm_captioning.py` to see the initialization of the model.\n",
    "\n",
    "We use the implementation from torchvision and put a very thin wrapper module for our use-case.\n",
    "You do not need to implement anything here — you should read and understand the module definition, available in `rnn_lstm_captioning.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pV0Lau_yDwX"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import ImageEncoder\n",
    "\n",
    "model = ImageEncoder(pretrained=True, verbose=True).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVAxU-jO6bhR"
   },
   "source": [
    "## Word embedding\n",
    "In deep learning systems, we commonly represent words using vectors. Each word of the vocabulary will be associated with a vector, and these vectors will be learned jointly with the rest of the system.\n",
    "\n",
    "Implement the `WordEmbedding` module to convert words (represented by integers) into vectors.\n",
    "Run the following to check your implementation. You should see an error on the order of `1e-7` or less. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础RNN：单步反向传播\n",
    "接着，为单个时间步的基础循环神经网络实现 `rnn_step_backward` 函数。运行以下代码以数值方式验证梯度实现的正确性，你应看到误差在 `1e-8` 或更小的数量级。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZuz2ieE6bhR"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import WordEmbedding\n",
    "\n",
    "N, T, V, D = 2, 4, 5, 3\n",
    "\n",
    "x = torch.tensor([[0, 3, 1, 2], [2, 1, 0, 3]]).long()\n",
    "W = torch.linspace(0, 1, steps=V * D, **to_double).view(V, D)\n",
    "\n",
    "# Copy custom weight vector for sanity check:\n",
    "model_emb = WordEmbedding(V, D).to(**to_double)\n",
    "model_emb.W_embed.data.copy_(W)\n",
    "out = model_emb(x)\n",
    "expected_out = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.0, 0.07142857, 0.14285714],\n",
    "            [0.64285714, 0.71428571, 0.78571429],\n",
    "            [0.21428571, 0.28571429, 0.35714286],\n",
    "            [0.42857143, 0.5, 0.57142857],\n",
    "        ],\n",
    "        [\n",
    "            [0.42857143, 0.5, 0.57142857],\n",
    "            [0.21428571, 0.28571429, 0.35714286],\n",
    "            [0.0, 0.07142857, 0.14285714],\n",
    "            [0.64285714, 0.71428571, 0.78571429],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"out error: \", rel_error(expected_out, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6Py13Ak6bhX",
    "tags": []
   },
   "source": [
    "## Temporal Softmax loss\n",
    "\n",
    "In an RNN language model, at every timestep we produce a score for each word in the vocabulary.\n",
    "This score is obtained by applying an affine transform to the hidden state (think `nn.Linear` module).\n",
    "We know the ground-truth word at each timestep, so we use a cross-entropy loss at each timestep.\n",
    "We sum the losses over time and average them over the minibatch.\n",
    "\n",
    "However there is one wrinkle: since we operate over minibatches and different captions may have different lengths, we append `<NULL>` tokens to the end of each caption so they all have the same length. We don't want these `<NULL>` tokens to count toward the loss or gradient, so in addition to scores and ground-truth labels our loss function also accepts a `ignore_index` that tells it which index in caption should be ignored when computing the loss.\n",
    "\n",
    "Implement the `temporal_softmax_loss` and run the following cell to check if the implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlFvgXtD6bhX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import temporal_softmax_loss\n",
    "\n",
    "\n",
    "def check_loss(N, T, V, p):\n",
    "    x = 0.001 * torch.randn(N, T, V)\n",
    "    y = torch.randint(V, size=(N, T))\n",
    "    mask = torch.rand(N, T)\n",
    "    y[mask > p] = 0\n",
    "\n",
    "    # YOUR_TURN: Implement temporal_softmax_loss\n",
    "    print(temporal_softmax_loss(x, y, NULL_index).item())\n",
    "\n",
    "\n",
    "check_loss(1000, 1, 10, 1.0)  # Should be about 2.00-2.11\n",
    "check_loss(1000, 10, 10, 1.0)  # Should be about 20.6-21.0\n",
    "check_loss(5000, 10, 10, 0.1)  # Should be about 2.00-2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础RNN：前向传播\n",
    "现在你已经实现了基础RNN单个时间步的前向和反向传播，接下来将这些模块组合起来，实现一个能够处理完整数据序列的RNN。首先，通过调用之前定义的 `rnn_step_forward` 函数来实现 `rnn_forward`。\n",
    "\n",
    "运行以下代码以检查你的实现。你应看到误差在 `1e-6` 或更小的量级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWrmaSZaUxqX"
   },
   "source": [
    "## Captioning Module\n",
    "\n",
    "Now we are wrapping everything into the captioning module. Implement the `CaptioningRNN` module by following its instructions.\n",
    "This modoule will have a generic structure for RNN, LST, and attention-based LSTM -- which we control by providing `cell_type` argument (one of `[\"rnn\", \"lstm\", \"attn\"]`),\n",
    "For now you only need to implement for the case where `cell_type=\"rnn\"`, you will come back to this module with other two cases later in this assignment.\n",
    "\n",
    "Also skip the inference function (`CaptioningRNN.sample`) for now -- only implement `__init__` and `forward`.\n",
    "Run the following to check your forward pass using a small test case; you should see difference on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8a71FL_6bhZ"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import CaptioningRNN\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, W, H = 10, 400, 30, 40\n",
    "word_to_idx = {\"<NULL>\": 0, \"cat\": 2, \"dog\": 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type=\"rnn\",\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "# Copy parameters for sanity check:\n",
    "for k, v in model.named_parameters():\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).view(*v.shape))\n",
    "\n",
    "images = torch.randn(N, 3, *IMAGE_SHAPE)\n",
    "captions = (torch.arange(N * T) % V).view(N, T)\n",
    "\n",
    "loss = model(images, captions).item()\n",
    "expected_loss = 150.6090393066\n",
    "\n",
    "print(\"loss: \", loss)\n",
    "print(\"expected loss: \", expected_loss)\n",
    "print(\"difference: \", rel_error(torch.tensor(loss), torch.tensor(expected_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YAOcQ4h6bhc"
   },
   "source": [
    "## Overfit small data\n",
    "\n",
    "To make sure that everything is working as expected, we can try to overfit this image captioning model to a small subset of data.\n",
    "\n",
    "We have implemented the `train_captioner` function which accepts the model and training data, and runs a simple training loop - passing data to model, collecting training loss, then calling `backward()` to obtain gradients. These gradients are optimized using the [AdamW optimizer](https://arxiv.org/abs/1711.05101) (supported by PyTorch).\n",
    "You can read its implementation in `a5_helper.py`. \n",
    "\n",
    "We will overfit on a subset of 50 examples.\n",
    "You should see a final loss of less than `0.5` and it should be done fairly quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础RNN：反向传播\n",
    "实现基础RNN的 `rnn_backward` 函数。该函数需对整个序列执行反向传播，并调用你之前定义的 `rnn_step_backward` 函数。最终误差应控制在 `1e-6` 或更小范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzhsGRzk6bhd"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = 50\n",
    "sample_idx = torch.linspace(0, num_train - 1, steps=small_num_train).long()\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# optimization arguments\n",
    "num_epochs = 80\n",
    "\n",
    "# create the image captioning model\n",
    "model = CaptioningRNN(\n",
    "    cell_type=\"rnn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "model = model.to(**to_float)\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    rnn_overfit, _ = train_captioner(\n",
    "        model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=OVR_BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiHsRysE6bhe"
   },
   "source": [
    "## Inference: Sampling Captions\n",
    "\n",
    "Unlike classification models, image captioning models behave very differently at training time and at test time.\n",
    "At training time, we have access to the ground-truth caption, so we feed ground-truth words as input to the RNN at each timestep.\n",
    "At test time, we sample from the distribution over the vocabulary at each timestep, and feed the sample as input to the RNN at the next timestep.\n",
    "\n",
    "Implement the `CaptioningRNN.sample` for test-time sampling. After doing so, run the following to train a captioning model and sample from the model on both training and validation data.\n",
    "\n",
    "### Train the image captioning model\n",
    "\n",
    "Now perform the training on the entire training set. You should see a final loss less than `2.0` and each epoch should take ~14s - 44s to run, depending on the GPU colab assigns you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXHnPuM_FU7k"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = num_train\n",
    "sample_idx = torch.randint(num_train, size=(small_num_train,))\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "rnn_model = CaptioningRNN(\n",
    "    cell_type=\"rnn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    rnn_model_submit, rnn_loss_submit = train_captioner(\n",
    "        rnn_model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=60,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN：使用 autograd 进行反向传播\n",
    "\n",
    "现在我们将完全依赖 PyTorch 的 autograd 模块（`torch.autograd`）来计算 RNN 的反向传播过程。  \n",
    "`torch.autograd` 提供了实现任意标量函数**自动微分**的类和函数。  \n",
    "它对现有代码的改动极小——只需将 `requires_grad=True` 的张量传入你之前编写的前向函数，然后在输出上调用 `.backward(gradient=grad)`，即可自动计算输入和权重的梯度。\n",
    "\n",
    "**注意：我们在上一次作业期间发布了 PyTorch API 使用指南笔记本，你可以在 Piazza 和课程网站上找到。**\n",
    "\n",
    "现在我们可以将手动实现的反向传播与 autograd 的结果进行对比。  \n",
    "请阅读以下单元格中的代码并运行，以比较你的实现与 `torch.autograd` 的结果。  \n",
    "你应得到小于 `1e-10` 的相对误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97xga3Q5GO8B"
   },
   "source": [
    "### Test-time sampling\n",
    "The samples on training data should be very good; the samples on validation data will probably make less sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rvt326nX6bhf"
   },
   "outputs": [],
   "source": [
    "from a5_helper import decode_captions\n",
    "\n",
    "\n",
    "rnn_model.eval()\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    sample_idx = torch.randint(\n",
    "        0, num_train if split == \"train\" else num_val, (VIS_BATCH_SIZE,)\n",
    "    )\n",
    "    sample_images = data_dict[split + \"_images\"][sample_idx]\n",
    "    sample_captions = data_dict[split + \"_captions\"][sample_idx]\n",
    "\n",
    "    # decode_captions is loaded from a5_helper.py\n",
    "    gt_captions = decode_captions(sample_captions, data_dict[\"vocab\"][\"idx_to_token\"])\n",
    "\n",
    "    generated_captions = rnn_model.sample(sample_images.to(DEVICE))\n",
    "    generated_captions = decode_captions(\n",
    "        generated_captions, data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "\n",
    "    for i in range(VIS_BATCH_SIZE):\n",
    "        plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"[{split}] RNN Generated: {generated_captions[i]}\\nGT: {gt_captions[i]}\"\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8Zd6FGPvMMa"
   },
   "source": [
    "# Image Captioning with LSTMs\n",
    "\n",
    "So far you have implemented a vanilla RNN and applied it to image captioning.\n",
    "Next we will implement LSTM and use it for image captioning.\n",
    "\n",
    "**LSTM** stands for [Long-Short Term Memory Networks](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory), a variant of vanilla Recurrent Neural Networks.\n",
    "Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradients caused by repeated matrix multiplication.\n",
    "LSTMs solve this problem by replacing the simple update rule of the vanilla RNN with a gating mechanism.\n",
    "\n",
    "**LSTM Update Rule:** Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$; the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
    "\n",
    "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "i = \\sigma(a_i) \\hspace{2pc}\n",
    "f = \\sigma(a_f) \\hspace{2pc}\n",
    "o = \\sigma(a_o) \\hspace{2pc}\n",
    "g = \\tanh(a_g)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
    "\n",
    "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
    "\n",
    "$$\n",
    "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n",
    "h_t = o\\odot\\tanh(c_t)\n",
    "$$\n",
    "\n",
    "where $\\odot$ is the elementwise product of vectors.\n",
    "\n",
    "In the rest of the notebook we will implement the LSTM update rule and apply it to the image captioning task.\n",
    "In the code, we assume that data is stored in batches so that $X_t \\in \\mathbb{R}^{N\\times D}$, and will work with *transposed* versions of the parameters: $W_x \\in \\mathbb{R}^{D \\times 4H}$, $W_h \\in \\mathbb{R}^{H\\times 4H}$ so that activations $A \\in \\mathbb{R}^{N\\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 模块\n",
    "\n",
    "现在我们可以将基础的 RNN 实现封装为一个 PyTorch 模块。  \n",
    "回顾之前的作业或教程——`nn.Module` 是 PyTorch 中所有神经网络模块的基类。有关其属性、函数和方法的更多细节，请参阅 [PyTorch 官方文档](https://pytorch.org/docs/stable/nn.html?highlight=module#torch.nn.Module)。\n",
    "\n",
    "简而言之，权重和偏置在 `__init__` 中声明，`forward` 函数会调用之前实现的 `rnn_forward` 函数。  \n",
    "反向传播函数无需手动实现，将完全由 `torch.autograd` 自动处理。  \n",
    "**我们已在 `RNN` 类中为你完成这部分代码，但强烈建议你仔细阅读理解其实现。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4DNkZYevMMc"
   },
   "source": [
    "## LSTM: step forward\n",
    "\n",
    "Implement the forward pass for a single timestep of an LSTM in the `LSTM.step_forward()` function.\n",
    "This should be similar to the `rnn_step_forward` function that you implemented above, but using the LSTM update rule instead.\n",
    "Since `LSTM` extends PyTorch `nn.Module`, you don't need to implement backward part!\n",
    "\n",
    "Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awLF_A5ZvMMd"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import LSTM\n",
    "\n",
    "\n",
    "N, D, H = 3, 4, 5\n",
    "x = torch.linspace(-0.4, 1.2, steps=N * D, **to_double).view(N, D)\n",
    "prev_h = torch.linspace(-0.3, 0.7, steps=N * H, **to_double).view(N, H)\n",
    "prev_c = torch.linspace(-0.4, 0.9, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-2.1, 1.3, steps=4 * D * H, **to_double).view(D, 4 * H)\n",
    "Wh = torch.linspace(-0.7, 2.2, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "b = torch.linspace(0.3, 0.7, steps=4 * H, **to_double)\n",
    "\n",
    "\n",
    "# Create module and copy weight tensors for sanity check:\n",
    "model = LSTM(D, H).to(**to_double)\n",
    "model.Wx.data.copy_(Wx)\n",
    "model.Wh.data.copy_(Wh)\n",
    "model.b.data.copy_(b)\n",
    "\n",
    "next_h, next_c = model.step_forward(x, prev_h, prev_c)\n",
    "\n",
    "expected_next_h = torch.tensor(\n",
    "    [\n",
    "        [0.24635157, 0.28610883, 0.32240467, 0.35525807, 0.38474904],\n",
    "        [0.49223563, 0.55611431, 0.61507696, 0.66844003, 0.7159181],\n",
    "        [0.56735664, 0.66310127, 0.74419266, 0.80889665, 0.858299],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "expected_next_c = torch.tensor(\n",
    "    [\n",
    "        [0.32986176, 0.39145139, 0.451556, 0.51014116, 0.56717407],\n",
    "        [0.66382255, 0.76674007, 0.87195994, 0.97902709, 1.08751345],\n",
    "        [0.74192008, 0.90592151, 1.07717006, 1.25120233, 1.42395676],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"next_h error: \", rel_error(expected_next_h, next_h))\n",
    "print(\"next_c error: \", rel_error(expected_next_c, next_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErgRQwwzvMMt"
   },
   "source": [
    "## LSTM: forward\n",
    "\n",
    "Implement the `LSTM.forward()` function to run an LSTM forward on an entire time-series of data.\n",
    "When you are done, run the following to check your implementation. You should see an error on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用于图像描述的RNN\n",
    "\n",
    "你将实现构建图像描述模型（`CaptioningRNN`类）所需的若干工具和层。\n",
    "\n",
    "## 图像特征提取\n",
    "\n",
    "图像描述模型的第一个关键组件是编码器，它接收图像并生成用于解码描述的特征。  \n",
    "此处我们使用小型的[RegNetX-400MF](https://pytorch.org/vision/stable/models.html#torchvision.models.regnet_x_400mf)作为主干网络，以便在Colab上以合理时间完成训练。该模型与之前作业中使用的检测器主干网络类似。\n",
    "\n",
    "它接收形状为`(B, C, H, W)`的图像批次，并输出最终层的空间特征，形状为`(B, C, H/32, W/32)`。  \n",
    "对于普通RNN和LSTM，我们使用平均池化后的特征（形状为`(B, C)`）来解码描述；而对于带注意力机制的LSTM，则通过学习注意力权重对空间特征进行聚合。  \n",
    "请查看`rnn_lstm_captioning.py`中的`ImageEncoder`方法，了解模型的初始化过程。\n",
    "\n",
    "我们使用torchvision的实现，并为其封装了一个轻量级模块以适配当前任务。  \n",
    "你无需在此处实现任何代码——但应阅读并理解`rnn_lstm_captioning.py`中该模块的定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_x-3BJiEvMMv"
   },
   "outputs": [],
   "source": [
    "N, D, H, T = 2, 5, 4, 3\n",
    "x = torch.linspace(-0.4, 0.6, steps=N * T * D, **to_double).view(N, T, D)\n",
    "h0 = torch.linspace(-0.4, 0.8, steps=N * H, **to_double).view(N, H)\n",
    "Wx = torch.linspace(-0.2, 0.9, steps=4 * D * H, **to_double).view(D, 4 * H)\n",
    "Wh = torch.linspace(-0.3, 0.6, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "b = torch.linspace(0.2, 0.7, steps=4 * H, **to_double)\n",
    "\n",
    "\n",
    "# Create module and copy weight tensors for sanity check:\n",
    "model = LSTM(D, H).to(**to_double)\n",
    "model.Wx.data.copy_(Wx)\n",
    "model.Wh.data.copy_(Wh)\n",
    "model.b.data.copy_(b)\n",
    "\n",
    "hn = model(x, h0)\n",
    "\n",
    "expected_hn = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.01764008, 0.01823233, 0.01882671, 0.0194232],\n",
    "            [0.11287491, 0.12146228, 0.13018446, 0.13902939],\n",
    "            [0.31358768, 0.33338627, 0.35304453, 0.37250975],\n",
    "        ],\n",
    "        [\n",
    "            [0.45767879, 0.4761092, 0.4936887, 0.51041945],\n",
    "            [0.6704845, 0.69350089, 0.71486014, 0.7346449],\n",
    "            [0.81733511, 0.83677871, 0.85403753, 0.86935314],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"hn error: \", rel_error(expected_hn, hn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92SStL_tvMNK"
   },
   "source": [
    "## LSTM captioning model\n",
    "\n",
    "Now that you have implemented the `LSTM` module, update the `CaptioningRNN` module (`__init__` and `forward` implementation method **ONLY**) to also handle the case where `self.cell_type` is `lstm`.\n",
    "**This should require adding less than 5 lines of code.**\n",
    "\n",
    "Once you have done so, run the following to check your implementation. You should see a difference on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNpiC4WSvMNL"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import CaptioningRNN\n",
    "\n",
    "N, D, W, H = 10, 400, 30, 40\n",
    "word_to_idx = {\"<NULL>\": 0, \"cat\": 2, \"dog\": 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "# YOUR_TURN: Implement CaptioningRNN for lstm\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type=\"lstm\",\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "for k, v in model.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).view(*v.shape))\n",
    "\n",
    "images = torch.linspace(\n",
    "    -3.0, 3.0, steps=(N * 3 * IMAGE_SHAPE[0] * IMAGE_SHAPE[1]), **to_float\n",
    ").view(N, 3, *IMAGE_SHAPE)\n",
    "captions = (torch.arange(N * T) % V).view(N, T)\n",
    "\n",
    "loss = model(images.to(DEVICE), captions.to(DEVICE))\n",
    "expected_loss = torch.tensor(146.3161468505)\n",
    "\n",
    "print(\"loss: \", loss.item())\n",
    "print(\"expected loss: \", expected_loss.item())\n",
    "print(\"difference: \", rel_error(loss, expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词嵌入\n",
    "在深度学习系统中，我们通常使用向量来表示词语。词汇表中的每个词都会对应一个向量，这些向量将与系统的其余部分一起联合学习。\n",
    "\n",
    "请实现 `WordEmbedding` 模块，将词语（以整数形式表示）转换为向量。\n",
    "运行以下代码以验证你的实现，预期误差应在 `1e-7` 或更小范围内。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06hbDnRXvMNO"
   },
   "source": [
    "## Overfit small data\n",
    "We have written this part for you. Run the following to overfit an LSTM captioning model on the same small dataset as we used for the RNN previously. You should see a final loss less than `4` after 80 epochs and it should run fairly quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-tETnd3vMNP"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# Data input.\n",
    "small_num_train = 50\n",
    "sample_idx = torch.linspace(0, num_train - 1, steps=small_num_train).long()\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx].to(DEVICE)\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx].to(DEVICE)\n",
    "\n",
    "# Create the image captioning model.\n",
    "model = CaptioningRNN(\n",
    "    cell_type=\"lstm\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "for learning_rate in [1e-2]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    lstm_overfit, _ = train_captioner(\n",
    "        model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=80,\n",
    "        batch_size=OVR_BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vzLUzlWvMNT"
   },
   "source": [
    "## Caption sampling\n",
    "\n",
    "Modify the  `CaptioningRNN.sample` method in class to handle the case where `self.cell_type` is `lstm`. **This should take fewer than 10 lines of code.**\n",
    "When you are done, run the following cells to train the captioning model first, then sample some captions from your model during test time.\n",
    "\n",
    "### Train the net\n",
    "\n",
    "Perform the training on the entire training set. You should see a final loss less than `1.8`. Each epoch should take ~7s - 14s to run, depending on the GPU\n",
    "colab assigns you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时序 Softmax 损失\n",
    "\n",
    "在 RNN 语言模型中，每个时间步都会为词汇表中的每个词生成一个得分，该得分通过对隐藏状态应用仿射变换（类似 `nn.Linear` 模块）获得。由于每个时间步都有对应的正确词，我们采用交叉熵损失计算每个时间步的损失，再将所有时间步的损失相加，并在小批量样本上取平均。\n",
    "\n",
    "但有一个细节需要注意：由于我们使用小批量处理，而不同句子长度可能不同，因此会在每个句子末尾填充 `<NULL>` 标记，使其长度一致。我们不希望这些 `<NULL>` 标记参与损失或梯度计算，因此除了得分和真实标签外，损失函数还接受一个 `ignore_index` 参数，用于指定计算损失时应忽略的索引。\n",
    "\n",
    "请实现 `temporal_softmax_loss` 函数，并运行下方代码单元以验证实现是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9MFRowdoHW7"
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = num_train\n",
    "sample_idx = torch.randint(num_train, size=(small_num_train,))\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "lstm_model = CaptioningRNN(\n",
    "    cell_type=\"lstm\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "lstm_model = lstm_model.to(DEVICE)\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    lstm_model_submit, lstm_loss_submit = train_captioner(\n",
    "        lstm_model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=60,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsM2pIYpG3v1"
   },
   "source": [
    "### Test-time sampling\n",
    "As with the RNN, the samples on training data should be very good; the samples on validation data will probably make less sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziQJ7SBnvMNU"
   },
   "outputs": [],
   "source": [
    "from a5_helper import decode_captions\n",
    "\n",
    "\n",
    "lstm_model.eval()\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    sample_idx = torch.randint(\n",
    "        0, num_train if split == \"train\" else num_val, (VIS_BATCH_SIZE,)\n",
    "    )\n",
    "    sample_images = data_dict[split + \"_images\"][sample_idx]\n",
    "    sample_captions = data_dict[split + \"_captions\"][sample_idx]\n",
    "\n",
    "    # decode_captions is loaded from a5_helper.py\n",
    "    gt_captions = decode_captions(sample_captions, data_dict[\"vocab\"][\"idx_to_token\"])\n",
    "    lstm_model.eval()\n",
    "    generated_captions = lstm_model.sample(sample_images.to(DEVICE))\n",
    "    generated_captions = decode_captions(\n",
    "        generated_captions, data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "\n",
    "    for i in range(VIS_BATCH_SIZE):\n",
    "        plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            f\"[{split}] LSTM Generated: {generated_captions[i]}\\nGT: {gt_captions[i]}\"\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标题生成模块\n",
    "\n",
    "现在我们将所有内容整合到标题生成模块中。请按照说明实现 `CaptioningRNN` 模块。该模块将具有通用结构，支持 RNN、LSTM 和基于注意力机制的 LSTM —— 我们通过传入 `cell_type` 参数（取值为 `[\"rnn\", \"lstm\", \"attn\"]` 之一）来控制具体使用的结构。目前你只需实现 `cell_type=\"rnn\"` 的情况，后续任务中再回来补充另外两种类型。\n",
    "\n",
    "暂时跳过推理函数（`CaptioningRNN.sample`），仅需实现 `__init__` 和 `forward` 方法。运行以下代码，使用一个小测试用例检查前向传播是否正确；你应看到误差在 `1e-7` 或更小量级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECcPPE_Pqc8v"
   },
   "source": [
    "# Attention LSTM\n",
    "Attention LSTM essentially adds an attention input $x_{attn}^t\\in\\mathbb{R}^H$ into LSTM, along with $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$.\n",
    "\n",
    "To get the attention input $x_{attn}^t$, here we adopt a method called `scaled dot-product attention`, as covered in the lecture. We first project the CNN feature activation from $\\mathbb{R}^{400\\times4\\times4}$ to $\\mathbb{R}^{H\\times4\\times4}$ using an affine layer. Given the projected activation $A\\in \\mathbb{R}^{H\\times4\\times4}$ and the LSTM hidden state from the previous time step $h_{t-1}$, we formuate the attention weights on $A$ at time step $t$ as $M_{attn}^t=h_{t-1}A/\\sqrt{H} \\in \\mathbb{R}^{4\\times4}$.\n",
    "\n",
    "To simplify the formuation here, we flatten the spatial dimensions of $A$ and $M_{attn}^t$ which gives $\\tilde{A}\\in \\mathbb{R}^{H\\times16}$ and $\\tilde{M^t}_{attn}=h_{t-1}A\\in \\mathbb{R}^{16}$.\n",
    "We add a **`softmax`** activation function on $\\tilde{M^t}_{attn}$ so that the attention weights at each time step are normalized and sum up to one.\n",
    "\n",
    "The attention embedding given the attention weights is then $x_{attn}^t=\\tilde{A}\\tilde{M^t}_{attn} \\in\\mathbb{R}^H$. Next, you will implement a batch version of the attention layer we have described here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTDk54Q4ubQ1"
   },
   "source": [
    "## Scaled dot-product attention\n",
    "Implement the `dot_product_attention` function. Given the LSTM hidden state from the previous time step `prev_h` (or $h_{t-1}$) and the projected CNN feature activation `A`, compute the attention weights `attn_weights` (or $\\tilde{M^t}_{attn}$ with a reshaping to $\\mathbb{R}^{4\\times4}$) attention embedding output `attn` (or $x_{attn}^t$) using the formulation we provided.\n",
    "\n",
    "When you are done, run the following to check your implementation. You should see an error on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irAslXWfaVGw"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import dot_product_attention\n",
    "\n",
    "\n",
    "N, H = 2, 5\n",
    "D_a = 4\n",
    "\n",
    "prev_h = torch.linspace(-0.4, 0.6, steps=N * H, **to_double).view(N, H)\n",
    "A = torch.linspace(-0.4, 1.8, steps=N * H * D_a * D_a, **to_double).view(\n",
    "    N, H, D_a, D_a\n",
    ")\n",
    "\n",
    "# YOUR_TURN: Implement dot_product_attention\n",
    "attn, attn_weights = dot_product_attention(prev_h, A)\n",
    "\n",
    "expected_attn = torch.tensor(\n",
    "    [\n",
    "        [-0.29784344, -0.07645979, 0.14492386, 0.36630751, 0.58769115],\n",
    "        [0.81412643, 1.03551008, 1.25689373, 1.47827738, 1.69966103],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "expected_attn_weights = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.06511126, 0.06475411, 0.06439892, 0.06404568],\n",
    "            [0.06369438, 0.06334500, 0.06299754, 0.06265198],\n",
    "            [0.06230832, 0.06196655, 0.06162665, 0.06128861],\n",
    "            [0.06095243, 0.06061809, 0.06028559, 0.05995491],\n",
    "        ],\n",
    "        [\n",
    "            [0.05717142, 0.05784357, 0.05852362, 0.05921167],\n",
    "            [0.05990781, 0.06061213, 0.06132473, 0.06204571],\n",
    "            [0.06277517, 0.06351320, 0.06425991, 0.06501540],\n",
    "            [0.06577977, 0.06655312, 0.06733557, 0.06812722],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"attn error: \", rel_error(expected_attn, attn))\n",
    "print(\"attn_weights error: \", rel_error(expected_attn_weights, attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在小数据集上过拟合\n",
    "\n",
    "为确保一切运行正常，我们可以尝试让这个图像描述模型在一小部分数据上过拟合。\n",
    "\n",
    "我们已实现 `train_captioner` 函数，该函数接收模型和训练数据，并执行一个简单的训练循环：将数据传入模型、收集训练损失，然后调用 `backward()` 计算梯度。这些梯度通过 [AdamW 优化器](https://arxiv.org/abs/1711.05101)（PyTorch 支持）进行优化。你可以在 `a5_helper.py` 中查看其实现。\n",
    "\n",
    "我们将使用 50 个样本的子集进行过拟合训练。最终损失应低于 `0.5`，且训练过程应相当迅速。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVddQlj4xwRk"
   },
   "source": [
    "## Attention LSTM: step forward\n",
    "\n",
    "Implement `AttentionLSTM.step_forward()` by following its instructions and input specifications.\n",
    "It is mostly similar to `LSTM.step_forward()` but has extra attention input `attn` (or $x_{attn}$) and its embedding weight matrix `Wattn` (or $W_{attn}$),\n",
    "these are defined in `AttentionLSTM.__init__()`.\n",
    "Hence, at each timestep the *activation vector* $a\\in\\mathbb{R}^{4H}$ in LSTM cell is formulated as:\n",
    "\n",
    "$a=W_xx_t + W_hh_{t-1}+W_{attn}x_{attn}^t+b$.\n",
    "\n",
    "\n",
    "**This should require adding less than 5 lines of code.**\n",
    "Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaS31Ncf3l0d"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import AttentionLSTM\n",
    "\n",
    "\n",
    "N, D, H = 3, 4, 5\n",
    "\n",
    "x = torch.linspace(-0.4, 1.2, steps=N * D, **to_double).view(N, D)\n",
    "prev_h = torch.linspace(-0.3, 0.7, steps=N * H, **to_double).view(N, H)\n",
    "prev_c = torch.linspace(-0.4, 0.9, steps=N * H, **to_double).view(N, H)\n",
    "attn = torch.linspace(0.6, 1.8, steps=N * H, **to_double).view(N, H)\n",
    "\n",
    "Wx = torch.linspace(-2.1, 1.3, steps=4 * D * H, **to_double).view(D, 4 * H)\n",
    "Wh = torch.linspace(-0.7, 2.2, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "b = torch.linspace(0.3, 0.7, steps=4 * H, **to_double)\n",
    "Wattn = torch.linspace(1.3, 4.2, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "\n",
    "# Create module and copy weight tensors for sanity check:\n",
    "model = AttentionLSTM(D, H).to(**to_double)\n",
    "model.Wx.data.copy_(Wx)\n",
    "model.Wh.data.copy_(Wh)\n",
    "model.b.data.copy_(b)\n",
    "model.Wattn.data.copy_(Wattn)\n",
    "\n",
    "next_h, next_c = model.step_forward(x, prev_h, prev_c, attn)\n",
    "\n",
    "\n",
    "expected_next_h = torch.tensor(\n",
    "    [\n",
    "        [0.53704256, 0.59980774, 0.65596820, 0.70569729, 0.74932626],\n",
    "        [0.78729857, 0.82010653, 0.84828362, 0.87235677, 0.89283167],\n",
    "        [0.91017981, 0.92483119, 0.93717126, 0.94754073, 0.95623746],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "expected_next_c = torch.tensor(\n",
    "    [\n",
    "        [0.59999328, 0.69285041, 0.78570758, 0.87856479, 0.97142202],\n",
    "        [1.06428558, 1.15714276, 1.24999992, 1.34285708, 1.43571424],\n",
    "        [1.52857143, 1.62142857, 1.71428571, 1.80714286, 1.90000000],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"next_h error: \", rel_error(expected_next_h, next_h))\n",
    "print(\"next_c error: \", rel_error(expected_next_c, next_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRry6hvQ7ywx"
   },
   "source": [
    "## Attention LSTM: forward\n",
    "\n",
    "Now, implement the `AttentinLSTM.forward()` function to run an attention-based LSTM on an entire timeseries of data.\n",
    "You will have to use the `dot_product_attention` function from outside this module.\n",
    "\n",
    "When you are done, run the following to check your implementation. You should see an error on the order of `1e-8` or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理：生成字幕采样\n",
    "\n",
    "与分类模型不同，图像字幕生成模型在训练阶段和测试阶段的行为差异很大。  \n",
    "训练时，我们能获取真实字幕，因此在每个时间步都将真实单词作为输入传给RNN。  \n",
    "测试时，我们在每个时间步从词汇表的概率分布中采样，并将采样结果作为下一个时间步的RNN输入。\n",
    "\n",
    "请实现 `CaptioningRNN.sample` 方法，用于测试阶段的采样。完成后，运行以下代码训练字幕生成模型，并在训练集和验证集上进行采样。\n",
    "\n",
    "### 训练图像字幕生成模型\n",
    "\n",
    "现在在整个训练集上进行训练。最终损失应低于 `2.0`，每个训练周期耗时约14至44秒（具体取决于Colab分配的GPU性能）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB6VU8nl4SmS"
   },
   "outputs": [],
   "source": [
    "N, D, H, T = 2, 5, 4, 3\n",
    "D_a = 4\n",
    "\n",
    "x = torch.linspace(-0.4, 0.6, steps=N * T * D, **to_double).view(N, T, D)\n",
    "A = torch.linspace(-0.4, 1.8, steps=N * H * D_a * D_a, **to_double).view(\n",
    "    N, H, D_a, D_a\n",
    ")\n",
    "\n",
    "Wx = torch.linspace(-0.2, 0.9, steps=4 * D * H, **to_double).view(D, 4 * H)\n",
    "Wh = torch.linspace(-0.3, 0.6, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "Wattn = torch.linspace(1.3, 4.2, steps=4 * H * H, **to_double).view(H, 4 * H)\n",
    "b = torch.linspace(0.2, 0.7, steps=4 * H, **to_double)\n",
    "\n",
    "\n",
    "# Create module and copy weight tensors for sanity check:\n",
    "model = AttentionLSTM(D, H).to(**to_double)\n",
    "model.Wx.data.copy_(Wx)\n",
    "model.Wh.data.copy_(Wh)\n",
    "model.b.data.copy_(b)\n",
    "model.Wattn.data.copy_(Wattn)\n",
    "\n",
    "# YOUR_TURN: Implement attention_forward\n",
    "hn = model(x, A)\n",
    "\n",
    "expected_hn = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.56141729, 0.70274849, 0.80000386, 0.86349400],\n",
    "            [0.89556391, 0.92856726, 0.94950579, 0.96281018],\n",
    "            [0.96792077, 0.97535465, 0.98039623, 0.98392994],\n",
    "        ],\n",
    "        [\n",
    "            [0.95065880, 0.97135490, 0.98344373, 0.99045552],\n",
    "            [0.99317679, 0.99607466, 0.99774317, 0.99870293],\n",
    "            [0.99907382, 0.99946784, 0.99969426, 0.99982435],\n",
    "        ],\n",
    "    ],\n",
    "    **to_double\n",
    ")\n",
    "\n",
    "print(\"h error: \", rel_error(expected_hn, hn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VzpyHuX6Jzc"
   },
   "source": [
    "## Attention LSTM captioning model\n",
    "\n",
    "With all your implementation done so far, you can finally update the implementation of `CaptioningRNN.__init__` and `CaptioningRNN.forward` methods once again.\n",
    "This time, handle the case where `self.cell_type` is `attn`. **This should require adding less than 10 lines of code.**\n",
    "\n",
    "Once you have done so, run the following to check your implementation. You should see a difference on the order of `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VqGqDYw6Jzd"
   },
   "outputs": [],
   "source": [
    "from rnn_lstm_captioning import CaptioningRNN\n",
    "\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "N, D, W, H = 10, 400, 30, 40\n",
    "word_to_idx = {\"<NULL>\": 0, \"cat\": 2, \"dog\": 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "# YOUR_TURN: Modify CaptioningRNN for attention\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type=\"attn\",\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "for k, v in model.named_parameters():\n",
    "    # print(k, v.shape) # uncomment this to see the weight shape\n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).view(*v.shape))\n",
    "\n",
    "images = torch.linspace(\n",
    "    -3.0, 3.0, steps=(N * 3 * IMAGE_SHAPE[0] * IMAGE_SHAPE[1])\n",
    ").view(N, 3, *IMAGE_SHAPE)\n",
    "captions = (torch.arange(N * T) % V).view(N, T)\n",
    "\n",
    "loss = model(images.to(DEVICE), captions.to(DEVICE))\n",
    "expected_loss = torch.tensor(8.0156393051)\n",
    "\n",
    "print(\"loss: \", loss.item())\n",
    "print(\"expected loss: \", expected_loss.item())\n",
    "print(\"difference: \", rel_error(loss, expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试时采样  \n",
    "训练数据上的样本应当表现良好；验证数据上的样本可能意义较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYxXTAn4q0wV"
   },
   "source": [
    "## Overfit small data\n",
    "We have written this part for you. Run the following to overfit an Attention LSTM captioning model on the same small dataset as we used for the RNN previously. You should see a final loss less than `9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlK7lKUgWeDS"
   },
   "outputs": [],
   "source": [
    "from a5_helper import train_captioner\n",
    "\n",
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = 50\n",
    "sample_idx = torch.linspace(0, num_train - 1, steps=small_num_train).long()\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "model = CaptioningRNN(\n",
    "    cell_type=\"attn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    attn_overfit, _ = train_captioner(\n",
    "        model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=80,\n",
    "        batch_size=OVR_BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ircMb7_qq7vB"
   },
   "source": [
    "## Caption sampling\n",
    "\n",
    "Modify the `CaptioningRNN.sample` method to handle the case where `self.cell_type` is `attn`. **This should take fewer than 10 lines of code.**\n",
    "\n",
    "When you are done run the following to train a captioning model and sample from the model on some training and validation set samples.\n",
    "\n",
    "### Train the net\n",
    "\n",
    "Now, perform the training on the entire training set. You should see a final loss less than `0.5`. Each epoch should take ~8s to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScBvAfcXdVv4"
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "\n",
    "# data input\n",
    "small_num_train = num_train\n",
    "sample_idx = torch.randint(num_train, size=(small_num_train,))\n",
    "small_image_data = data_dict[\"train_images\"][sample_idx]\n",
    "small_caption_data = data_dict[\"train_captions\"][sample_idx]\n",
    "\n",
    "# create the image captioning model\n",
    "attn_model = CaptioningRNN(\n",
    "    cell_type=\"attn\",\n",
    "    word_to_idx=data_dict[\"vocab\"][\"token_to_idx\"],\n",
    "    input_dim=400,  # hard-coded, do not modify\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    ignore_index=NULL_index,\n",
    ")\n",
    "attn_model = attn_model.to(DEVICE)\n",
    "\n",
    "for learning_rate in [1e-3]:\n",
    "    print(\"learning rate is: \", learning_rate)\n",
    "    attn_model_submit, attn_loss_submit = train_captioner(\n",
    "        attn_model,\n",
    "        small_image_data,\n",
    "        small_caption_data,\n",
    "        num_epochs=60,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=learning_rate,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM：单步前向传播\n",
    "\n",
    "在 `LSTM.step_forward()` 函数中实现 LSTM 单个时间步的前向传播。该实现应与你之前完成的 `rnn_step_forward` 函数类似，但需改用 LSTM 的更新规则。由于 `LSTM` 继承自 PyTorch 的 `nn.Module`，你无需手动实现反向传播部分！\n",
    "\n",
    "完成后，运行以下代码对你的实现进行简单测试。你应看到误差在 `1e-7` 或更小量级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ham_O1TG_z7"
   },
   "source": [
    "### Test-time sampling and visualization\n",
    "As with RNN and LSTM, the samples on training data should be very good; the samples on validation data will probably make less sense.\n",
    "\n",
    "We use the `attention_visualizer` function from `eecs598/utils.py` to visualize the attended regions per generated word. Note that sometimes the attended regions (brighter) might not make much sense particially due to our low resolution image input. In real applications, the attended regions are more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i8KNWSDSLNu"
   },
   "outputs": [],
   "source": [
    "# Sample a minibatch and show the reshaped 112x112 images,\n",
    "# GT captions, and generated captions by your model.\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    sample_idx = torch.randint(\n",
    "        0, num_train if split == \"train\" else num_val, (VIS_BATCH_SIZE,)\n",
    "    )\n",
    "    sample_images = data_dict[split + \"_images\"][sample_idx]\n",
    "    sample_captions = data_dict[split + \"_captions\"][sample_idx]\n",
    "\n",
    "    # decode_captions is loaded from a5_helper.py\n",
    "    gt_captions = decode_captions(sample_captions, data_dict[\"vocab\"][\"idx_to_token\"])\n",
    "    attn_model.eval()\n",
    "    generated_captions, attn_weights_all = attn_model.sample(sample_images.to(DEVICE))\n",
    "    generated_captions = decode_captions(\n",
    "        generated_captions, data_dict[\"vocab\"][\"idx_to_token\"]\n",
    "    )\n",
    "\n",
    "    for i in range(VIS_BATCH_SIZE):\n",
    "        plt.imshow(sample_images[i].permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\n",
    "            \"%s\\nAttention LSTM Generated:%s\\nGT:%s\"\n",
    "            % (split, generated_captions[i], gt_captions[i])\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "        tokens = generated_captions[i].split(\" \")\n",
    "\n",
    "        vis_attn = []\n",
    "        for j in range(len(tokens)):\n",
    "            img = sample_images[i]\n",
    "            attn_weights = attn_weights_all[i][j]\n",
    "            token = tokens[j]\n",
    "            img_copy = attention_visualizer(img, attn_weights, token)\n",
    "            vis_attn.append(transforms.ToTensor()(img_copy))\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (20.0, 20.0)\n",
    "        vis_attn = make_grid(vis_attn, nrow=8)\n",
    "        plt.imshow(torch.flip(vis_attn, dims=(0,)).permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghB8BwfUpmI5"
   },
   "source": [
    "# Save results for submission\n",
    "\n",
    "Once you have finished all your implementation, run \"Runtime -> Restart and run all...\" to re-run all cells and display outputs.\n",
    "Make sure all outputs are displayed properly and the outputs are same as what you expected!\n",
    "\n",
    "Once all the cells are completed, execute the following cell to save the final losses for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM：前向传播\n",
    "\n",
    "请实现 `LSTM.forward()` 函数，以在整个时间序列数据上执行 LSTM 的前向传播。完成后，运行以下代码检查你的实现。你应看到误差在 `1e-7` 或更小的数量级。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_21vKQeRMpC"
   },
   "outputs": [],
   "source": [
    "submission = {\n",
    "    \"rnn_losses\": rnn_loss_submit,\n",
    "    \"lstm_losses\": lstm_loss_submit,\n",
    "    \"attn_losses\": attn_loss_submit,\n",
    "}\n",
    "submission_path = os.path.join(GOOGLE_DRIVE_PATH, \"rnn_lstm_attention_submission.pt\")\n",
    "torch.save(submission, submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存结果以供提交\n",
    "\n",
    "完成所有实现后，请运行“运行时 -> 重启并运行所有...”以重新执行所有单元格并显示输出。  \n",
    "请确保所有输出均正确显示，且与预期结果一致！\n",
    "\n",
    "所有单元格完成后，执行以下单元格以保存最终损失值用于提交。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试时采样与可视化\n",
    "与RNN和LSTM类似，训练数据上的采样结果应当非常理想；而在验证数据上的采样结果可能意义较弱。\n",
    "\n",
    "我们使用 `eecs598/utils.py` 中的 `attention_visualizer` 函数，可视化每个生成词所关注的图像区域。需要注意的是，有时关注区域（较亮部分）可能不太合理，部分原因在于我们输入的图像分辨率较低。在实际应用中，关注区域通常更为准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 图像描述模型\n",
    "\n",
    "既然你已经实现了 `LSTM` 模块，请更新 `CaptioningRNN` 模块（仅修改 `__init__` 和 `forward` 方法），使其也能处理 `self.cell_type` 为 `lstm` 的情况。  \n",
    "**此操作应只需添加不到 5 行代码。**\n",
    "\n",
    "完成后，运行以下代码以验证你的实现。你应看到差异在 `1e-7` 或更小量级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采样标题\n",
    "\n",
    "修改 `CaptioningRNN.sample` 方法，使其能够处理 `self.cell_type` 为 `attn` 的情况。**代码应少于10行。**\n",
    "\n",
    "完成后，运行以下代码训练一个标题生成模型，并在部分训练集和验证集样本上进行采样。\n",
    "\n",
    "### 训练网络\n",
    "\n",
    "现在，在整个训练集上进行训练。最终损失应低于 `0.5`，每个轮次耗时约8秒。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在小数据集上过拟合\n",
    "我们已为你编写了这部分代码。请运行以下代码，在与之前RNN所用相同的小数据集上对Attention LSTM字幕生成模型进行过拟合。最终损失应小于 `9`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制 LSTM 图像描述模型\n",
    "\n",
    "完成上述所有实现后，你可以再次更新 `CaptioningRNN.__init__` 和 `CaptioningRNN.forward` 方法的实现。  \n",
    "这次需处理 `self.cell_type` 为 `attn` 的情况，**新增代码应不超过 10 行**。\n",
    "\n",
    "完成后，运行以下代码以验证实现。你应看到误差在 `1e-7` 或更小量级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在小数据集上过拟合\n",
    "这部分我们已为你写好。请运行以下代码，在与之前RNN所用相同的小数据集上对LSTM字幕生成模型进行过拟合训练。经过80个轮次后，你应看到最终损失低于4，且训练速度相当快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力LSTM：前向传播\n",
    "\n",
    "现在，请实现 `AttentionLSTM.forward()` 函数，以便在整个时间序列数据上运行基于注意力机制的LSTM。你需要使用本模块外部的 `dot_product_attention` 函数。\n",
    "\n",
    "完成后，运行以下代码以验证你的实现。你应看到误差在 `1e-8` 或更小的数量级。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention LSTM：前向步进\n",
    "\n",
    "请根据说明和输入规范实现 `AttentionLSTM.step_forward()`。  \n",
    "它与 `LSTM.step_forward()` 大体相似，但额外接收注意力输入 `attn`（或 $x_{attn}$）及其嵌入权重矩阵 `Wattn`（或 $W_{attn}$），这些已在 `AttentionLSTM.__init__()` 中定义。  \n",
    "因此，在每个时间步，LSTM 单元中的*激活向量* $a\\in\\mathbb{R}^{4H}$ 计算公式为：\n",
    "\n",
    "$a=W_xx_t + W_hh_{t-1}+W_{attn}x_{attn}^t+b$。\n",
    "\n",
    "**此部分只需添加不超过 5 行代码。**  \n",
    "完成后，请运行以下代码对你的实现进行简单测试，预期误差应在 `1e-8` 或更小范围内。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缩放点积注意力\n",
    "实现 `dot_product_attention` 函数。根据上一时间步的 LSTM 隐藏状态 `prev_h`（即 $h_{t-1}$）和投影后的 CNN 特征激活 `A`，利用我们提供的公式计算注意力权重 `attn_weights`（即 $\\tilde{M^t}_{attn}$，需重塑为 $\\mathbb{R}^{4\\times4}$ 形状）以及注意力嵌入输出 `attn`（即 $x_{attn}^t$）。\n",
    "\n",
    "完成后，运行以下代码验证你的实现。预期误差应约为 `1e-7` 或更小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标题采样\n",
    "\n",
    "修改 `CaptioningRNN.sample` 方法，使其支持 `self.cell_type` 为 `lstm` 的情况。**代码应少于10行。**\n",
    "完成后，运行以下单元格，先训练标题生成模型，然后在测试阶段从模型中采样生成一些标题。\n",
    "\n",
    "### 训练网络\n",
    "\n",
    "在完整训练集上进行训练，最终损失应低于 `1.8`。根据 Colab 分配的 GPU 不同，每轮训练耗时约 7 至 14 秒。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力LSTM\n",
    "\n",
    "注意力LSTM本质上是在标准LSTM的基础上，额外引入一个注意力输入 $x_{attn}^t\\in\\mathbb{R}^H$，与原始输入 $x_t\\in\\mathbb{R}^D$ 和前一时刻的隐藏状态 $h_{t-1}\\in\\mathbb{R}^H$ 一同参与计算。\n",
    "\n",
    "为了获得注意力输入 $x_{attn}^t$，我们采用课堂中介绍的“缩放点积注意力”方法。首先，通过一个仿射层将CNN特征激活从 $\\mathbb{R}^{400\\times4\\times4}$ 投影到 $\\mathbb{R}^{H\\times4\\times4}$，得到投影后的特征张量 $A\\in \\mathbb{R}^{H\\times4\\times4}$。结合前一时刻的LSTM隐藏状态 $h_{t-1}$，我们在第 $t$ 步计算注意力权重 $M_{attn}^t=h_{t-1}A/\\sqrt{H} \\in \\mathbb{R}^{4\\times4}$。\n",
    "\n",
    "为简化表达，我们将 $A$ 和 $M_{attn}^t$ 的空间维度展平，得到 $\\tilde{A}\\in \\mathbb{R}^{H\\times16}$ 和 $\\tilde{M^t}_{attn}=h_{t-1}A\\in \\mathbb{R}^{16}$。接着对 $\\tilde{M^t}_{attn}$ 应用 **`softmax`** 激活函数，确保每个时间步上的注意力权重归一化且总和为1。\n",
    "\n",
    "最终，基于注意力权重计算得到的注意力嵌入为 $x_{attn}^t=\\tilde{A}\\tilde{M^t}_{attn} \\in\\mathbb{R}^H$。接下来，你将实现上述注意力层的批处理版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试时采样\n",
    "与RNN一样，训练数据上的样本应表现很好；验证数据上的样本可能意义较小。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_lstm_attention_captioning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
